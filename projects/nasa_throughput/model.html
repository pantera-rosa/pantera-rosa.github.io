<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>FlightHunters</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <nav>
      <ul>
        <li><a href="index.html">Home</a></li>
        <li class="dropdown">
          <a href="#" class="dropbtn">Data</a>
          <div class="dropdown-content">
            <a href="data.html">Overview</a>
            <a href="cwam.html">CWAM</a>
            <a href="taf.html">TAF</a>
            <a href="metar.html">METAR</a>
            <a href="fuser.html">FUSER</a>
          </div>
        </li>
        <li class="dropdown">
          <a href="" class="dropbtn">Model</a>
          <div class="dropdown-content">
            <a href="model.html">Design</a>
            <a href="results.html">Results</a>
          </div>
        </li>
        <li><a href="references.html">References</a></li>
      </ul>
    </nav>
    <main>
      <h1>Model Design</h1>
      <div class="content">
        <h3>Model</h3>
        <h4>Implementation</h4>
        <p>
          The data we were provided is in a time series format. In order to
          account for the dimension of time, we utilized a recurrent neural
          network. RNNs capture time steps in hidden layers, which is critical
          when trying to analyze how data evolves with respect to temporal
          conditions. In our case, we are dealing with both short and long term
          time dependencies, where we need to not just capture micro changes per
          hour but also macro changes like meteorological differences per
          season. For this, we built an LSTM to capture these long term
          dependencies specifically.
        </p>
        <p>
          The training/validation split was 80/20, stratified by airport and
          month. Our model consists of the following:
        </p>
        <ol>
          <li>Batch Normalization Layer: to normalize all features</li>
          <li>
            Time Distributed Layer: relu activation to capture non-linear
            patterns with an L2 regularizer which penalizes large weights and
            prevents overfitting
          </li>
          <li>
            LSTM Layers: to capture long term dependencies, going down in number
            of units as each layer is applied to refine patterns, with a dropout
            set to prevent overfitting
          </li>
          <li>
            Dropout Layer: prevents overfitting and improves generalization
          </li>
        </ol>

        <h4>Challenges</h4>
        <p>
          Our original model implementation returned mainly the average values
          for thoroughput for each day, acting more like a baseline model than a
          model that has learned anything of significance. This failure was due
          to the depiction of time as linear instead of cyclic; by taking the
          sine and cosine values of days and hours, we were able to
        </p>

        <h3>Hyperparameter Tuning</h3>
        <p>Results:</p>
        <ul>
          <li>
            Learning rate: [<u><b>1e-4</b></u
            >, 1e-5, 1e-6]
          </li>
          <li>
            Batch size: [<u><b>16</b></u
            >, 32]
          </li>
          <li>
            Dense layer dropout rate: [0.1, <u><b>0.2</b></u
            >, 0.3]
          </li>
          <li>
            LSTM layer dropout rate: [0.1, <u><b>0.2</b></u
            >]
          </li>
          <li>
            L2 Regularization factor: [<u><b>0.01</b></u
            >, 0.02]
          </li>
          <li>
            Dense layer units :[<u><b>32</b></u
            >, 64,128]
          </li>
          <li>
            LSTM units: [16, <u><b>32</b></u
            >, 64]
          </li>
        </ul>
      </div>
    </main>
  </body>
</html>
